---
layout: default
---

&nbsp;

[Proceedings 2019 - Part 1](/schedule/images/Proceedings2019_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2019 - Part 2](/schedule/images/Proceedings2019_Part2.pdf)

[Proceedings 2020 - Part 1](/schedule/images/Proceedings2020_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2020 - Part 2](/schedule/images/Proceedings2020_Part2.pdf)

[Proceedings 2021 - Part 1](/schedule/images/Proceedings2021_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2021 - Part 2](/schedule/images/Proceedings2021_Part2.pdf)

&nbsp;

The course project can be categorized as a literature review, original research, or a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes an in-depth review and analysis of a paper (to be selected from a list of papers provided by the instructor or you after the instructor's approval). The review should provide an in-depth summary, exposition, and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current study with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner as possible**. **Deadline: February 2nd (Friday).**

- Submit a one-page description of the project, what it is about, your opinion, what needs to be done (related papers to read), and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan for the next steps and what needs to be done by the end of the semester to finish the project. **Deadline: February 16th (Friday).**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (~5-10mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The report should be at least six pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester if it deserves publication.

### Suggested list of projects/papers (to be updated)

**Project ideas**
  
- Computer-assisted worst-case analysis of gradient-based algorithms
  - [An optimal gradient method for smooth strongly convex minimization](https://arxiv.org/pdf/2101.09741.pdf)
  - [Counter-Examples in First-Order Optimization: A Constructive Approach](https://arxiv.org/pdf/2303.10503.pdf)
  - [On Fundamental Proof Structures in First-Order Optimization](https://arxiv.org/pdf/2310.02015.pdf)
  - [PEPit: computer-assisted worst-case analyses of first-order optimization methods in Python](https://arxiv.org/pdf/2201.04040.pdf)

- Modernized view of learning rates in optimization methods
  - [Acceleration via fractal learning rate schedules](https://proceedings.mlr.press/v139/agarwal21a/agarwal21a.pdf)
  - [Super-acceleration with cyclical step-sizes](https://proceedings.mlr.press/v151/goujaud22a/goujaud22a.pdf)
  - [Provably Faster Gradient Descent via Long Steps](https://arxiv.org/pdf/2307.06324.pdf)

- Recent advances in adaptive methods in ML
  - [Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient](https://arxiv.org/pdf/2301.04431.pdf)
  - [Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence](https://arxiv.org/pdf/2002.10542.pdf)
  - [Learning-Rate-Free Learning by D-Adaptation](https://arxiv.org/pdf/2301.07733.pdf)
  - [Automatic Gradient Descent: Deep Learning without Hyperparameters](https://arxiv.org/pdf/2304.05187.pdf)
  - [Adaptive FL with auto-tuned clients](https://arxiv.org/pdf/2306.11201.pdf)
  
- Sparse post-training pruning and update in Neural Network training
  - [i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery](https://arxiv.org/pdf/2112.04905.pdf)
  - [Writeup provided by instructor - per request]()

- Quantum Approximate Optimization Algorithm from a Optimization perspective
  - [Writeup provided by instructor - per request]()

- Transformer alternatives
  - [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  - [Overview of transformers](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
  - [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf)
  - [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf)
 
- (Recent) advances in asynchrony on distributed SGD
  - [Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays](https://arxiv.org/pdf/2106.12261.pdf)
  - [Asynchronous Stochastic Optimization Robust to Arbitrary Delays](https://proceedings.neurips.cc/paper/2021/file/4b85256c4881edb6c0776df5d81f6236-Paper.pdf)
  - [Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning](https://arxiv.org/pdf/2206.08307.pdf)
  - [Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays](https://arxiv.org/pdf/2206.07638.pdf)
 
- Review of classical continual learning methods
  - [iCaRL: Incremental Classifier and Representation Learning](https://arxiv.org/pdf/1611.07725.pdf)
  - [End-to-End Incremental Learning](https://arxiv.org/pdf/1807.09536.pdf)
  - [Efficient Lifelong Learning with A-GEM](https://arxiv.org/pdf/1812.00420.pdf)
  - [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/pdf/2004.10964.pdf)
  - [Gradient Episodic Memory for Continual Learning](https://arxiv.org/pdf/1706.08840.pdf)
  - [Cold Start Streaming Learning for Deep Networks](https://arxiv.org/pdf/2211.04624.pdf)

- Review of modern continual learning methods
  - [One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning](https://arxiv.org/pdf/2311.12048.pdf)
  - [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](https://arxiv.org/pdf/2305.11186.pdf)
  - [FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts](https://arxiv.org/pdf/2306.08586.pdf)
  - [Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation](https://arxiv.org/pdf/2310.02842.pdf)

- Review of ML models on weather forecasting
  - []()
 
- Review of large-scale ML models in AI
  - [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)
  - [Branch-Train-Merge training of expert LLMs](https://arxiv.org/pdf/2208.03306.pdf)
  - [Training a language model on a single GPU in one day](https://arxiv.org/pdf/2212.14034.pdf)
  - [Scaling ViT to 22B](https://arxiv.org/pdf/2302.05442.pdf)
  - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
  - [BloombergGPT](https://arxiv.org/pdf/2303.17564.pdf)
  - [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)
  - (probably more papers will be needed...)

- Literature review on adapters in neural network training and optimization
  - [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
  - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
  - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
  - [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf)

- Literature review of recent developments on Frank-Wolfe methods
  - [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
  - (More papers to be announced)

  
      
&nbsp;
&nbsp;
