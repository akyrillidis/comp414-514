---
layout: default
---

&nbsp;

[Proceedings 2019 - Part 1](/schedule/images/Proceedings2019_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2019 - Part 2](/schedule/images/Proceedings2019_Part2.pdf)

[Proceedings 2020 - Part 1](/schedule/images/Proceedings2020_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2020 - Part 2](/schedule/images/Proceedings2020_Part2.pdf)

[Proceedings 2021 - Part 1](/schedule/images/Proceedings2021_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2021 - Part 2](/schedule/images/Proceedings2021_Part2.pdf)

&nbsp;

The course project can be categorized as a literature review, as original research, or as a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes depth review and analysis of a paper (to be selected from a list of papers, provided by the instructor or by you, after instroctor's approval). The review should provide an in-depth summary, exposition and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current research with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner possible**. **Deadline: September 27th.**

- Submit a one-page description of the project, what is it about, your opinion, what needs to be done (related papers to read) and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan on what are the next steps and what needs to be done till the end of the semester to finish the project. **Deadline: October 6th.**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (~5-10mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The length of the report should be at least 6 pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester, if it deserves publication.

### Suggested list of projects/papers (to be updated)

**Project ideas**
  
- Practical algorithms for SDPs
  - [Smoothed analysis for low-rank solutions to SDPs in quadratic penalty form](https://arxiv.org/pdf/1803.00186.pdf)
  - [The mixing method - low-rank coordinate descent for semidefinite programming with diagonal constraints](https://arxiv.org/pdf/1706.00476.pdf)

- True alternating gradient descent for matrix factorization
  - [Near-optimal Local Convergence of Alternating Gradient Descent-Ascent for Minimax Optimization](https://arxiv.org/pdf/2102.09468.pdf)

- Faster algorithms in Sparsity in Neural Network training
  - [i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery](https://arxiv.org/pdf/2112.04905.pdf)
  - [IHT dies hard: Provable accelerated Iterative Hard Thresholding](https://arxiv.org/pdf/1712.09379.pdf)

- Sparse training + pruning in Neural Network training
  - [i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery](https://arxiv.org/pdf/2112.04905.pdf)
  - [Writeup provided by instructor - per request]()

- Quantum Approximate Optimization Algorithm from a Optimization perspective
  - [Writeup provided by instructor - per request]()

- A new min-max toy objective for non-convex/non-concave gradient descent characterization
  - [Writeup provided by instructor - per request]()

**Literature reviews**

- Literature review on adapters in neural network training and optimization
  - [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
  - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
  - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
  - [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf)

- Literature review of recent developments on Frank-Wolfe methods
  - [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
  - (More papers to be announced)
  
- (Recent) literature review on asynchrony on SGD
  - [Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays](https://arxiv.org/pdf/2106.12261.pdf)
  - [Asynchronous Stochastic Optimization Robust to Arbitrary Delays](https://proceedings.neurips.cc/paper/2021/file/4b85256c4881edb6c0776df5d81f6236-Paper.pdf)
  - [Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning](https://arxiv.org/pdf/2206.08307.pdf)
  - [Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays](https://arxiv.org/pdf/2206.07638.pdf)

- Low rank operations in modern ML
  - [Gradient descent happens in a tiny subspace](https://arxiv.org/pdf/1812.04754.pdf)
  - [Low-rank Gradient Approximation For Memory-Efficient On-device Training of Deep Neural Network](https://arxiv.org/pdf/2001.08885.pdf)
  - [Pufferfish: Communication-efficient Models At No Extra Cost](https://arxiv.org/pdf/2103.03936.pdf)
  - [Gradient Descent for Low-Rank Functions](https://arxiv.org/pdf/2206.08257.pdf)
  - [Attention is not all you need: pure attention loses rank doubly exponentially with depth](https://arxiv.org/pdf/2103.03404.pdf)

- The Lottery Ticket Hypothesis: Architectures and Theory
  - [The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635.pdf)
  - [Lottery Tickets on a Data Diet: Finding Initializations with Sparse Trainable Networks](https://arxiv.org/pdf/2206.01278.pdf)
  - [The Lottery Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_The_Lottery_Tickets_Hypothesis_for_Supervised_and_Self-Supervised_Pre-Training_in_CVPR_2021_paper.pdf)
  - [The Lottery Ticket Hypothesis for Pre-Trained BERT Networks](https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf)
  - [Whatâ€™s Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/pdf/1911.13299.pdf)
  - [Optimal Lottery Tickets via SubsetSum: Logarithmic Over-Parameterization is Sufficient](https://proceedings.neurips.cc/paper/2020/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf)
  - [Data-Efficient Structured Pruning via Submodular Optimization](https://arxiv.org/pdf/2203.04940.pdf)


- ~~Mixture of experts~~
  - ~~[Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/pdf/1701.06538.pdf)~~
  - ~~[Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/pdf/2101.03961.pdf)~~

- ~~Two regimes in neural network training~~
  - ~~[Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://arxiv.org/pdf/2103.00065.pdf)~~
  - ~~[Super-Convergence with an Unstable Learning Rate](https://arxiv.org/pdf/2102.10734.pdf)~~
  - ~~[The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/pdf/2003.02218.pdf)~~
  - ~~[The Two Regimes of Deep Network Training](https://arxiv.org/pdf/2002.10376.pdf)~~

  
      
&nbsp;
&nbsp;
