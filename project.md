---
layout: default
---

&nbsp;

[Proceedings 2019 - Part 1](/schedule/images/Proceedings2019_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2019 - Part 2](/schedule/images/Proceedings2019_Part2.pdf)

[Proceedings 2020 - Part 1](/schedule/images/Proceedings2020_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2020 - Part 2](/schedule/images/Proceedings2020_Part2.pdf)

&nbsp;

The course project can be categorized as a literature review, as original research, or as a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes depth review and analysis of a paper (to be selected from a list of papers, provided by the instructor or by you, after instroctor's approval). The review should provide an in-depth summary, exposition and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current research with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner possible**. **Deadline: September 21st.**

- Submit a one-page description of the project, what is it about, your opinion, what needs to be done (related papers to read) and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan on what are the next steps and what needs to be done till the end of the semester to finish the project. **Deadline: October 5th.**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (~5mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The length of the report should be at least 6 pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester, if it deserves publication.

### Suggested list of projects/papers (to be updated)

**Project ideas**

- Theory on simplistic ResNet models
  - [On the Global Convergence of Training Deep Linear ResNets](https://openreview.net/pdf?id=HJxEhREKDH)
  - [Identity Matters in Deep Learning](https://arxiv.org/pdf/1611.04231.pdf)
  
- Practical algorithms for SDPs
  - [Smoothed analysis for low-rank solutions to SDPs in quadratic penalty form](https://arxiv.org/pdf/1803.00186.pdf)
  - [The mixing method - low-rank coordinate descent for semidefinite programming with diagonal constraints](https://arxiv.org/pdf/1706.00476.pdf)

- Matrix completion for lower communication footprint
  - [Pufferfish: Communication-efficient Models At No Extra Cost](https://proceedings.mlsys.org/paper/2021/file/84d9ee44e457ddef7f2c4f25dc8fa865-Paper.pdf)
  - [PowerSGD: Practical Low-Rank Gradient Compression for Distributed Optimization](https://arxiv.org/pdf/1905.13727.pdf)

- True alternating gradient descent for matrix factorization
  - [Near-optimal Local Convergence of Alternating Gradient Descent-Ascent for Minimax Optimization](https://arxiv.org/pdf/2102.09468.pdf)

**Literature reviews**

- Recent algorithmic advances on explainable AI + Generative Additve Models (+ alternative approaches)
  - [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/pdf/2004.13912.pdf)
  - [Interpretable Learning-to-Rank with Generalized Additive Models](https://arxiv.org/pdf/2005.02553.pdf)
  - [Distributed Learning of Neural Networks using Independent Subnet Training](https://arxiv.org/pdf/1910.02120.pdf)

- Schedules in deep learning: from learning rates and momentum to quantization
  - [CPT: Efficient Deep Neural Network Training via Cyclic Precision](https://arxiv.org/pdf/2101.09868.pdf) - and references there in.
  - [Demon: Improved Neural Network Training with Momentum Decay](https://arxiv.org/pdf/1910.04952.pdf)
  - [REX: Revisiting Budgeted Training with an Improved Schedule](https://arxiv.org/pdf/2107.04197.pdf)
  - [Super-Acceleration with Cyclical Step-sizes](https://arxiv.org/pdf/2106.09687.pdf)

- ~~Deep randomized ML models: Kitchen Sinks, Random weights and pruning~~
  - ~~[Weighted Sums of Random Kitchen Sinks: Replacing minimization with randomization in learning](https://papers.nips.cc/paper/2008/file/0efe32849d230d7f53049ddc4a4b0c60-Paper.pdf)~~
  - ~~[What's Hidden in a Randomly Weighted Neural Network?](https://arxiv.org/pdf/1911.13299.pdf)~~
  - ~~[The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks](https://arxiv.org/pdf/1803.03635.pdf)~~

- ~~The use of momentum in stochastic methods~~
  - ~~[Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization](https://proceedings.icml.cc/static/paper_files/icml/2020/4205-Paper.pdf)~~
  - ~~[On the convergence of the Stochastic Heavy Ball Method](https://arxiv.org/pdf/2006.07867.pdf)~~
  - ~~[On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings](https://arxiv.org/pdf/2002.12414.pdf)~~

- ~~Literature review on the Lottery Ticket Hypothesis phenomenon~~

- Literature review of recent developments on Frank-Wolfe methods
  - [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
  - (More papers to be announced)
  
- Mixture of experts
  - [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/pdf/1701.06538.pdf)
  - [Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity](https://arxiv.org/pdf/2101.03961.pdf)

- Neural network training and convex optimization
  - [Convex Optimization for Shallow Neural Networks](https://proceedings.allerton.csl.illinois.edu/media/files/0140.pdf)
  - [Global Convergence of Frank Wolfe on One Hidden Layer Networks](https://arxiv.org/pdf/2002.02208.pdf)
  - [Revealing the Structure of Deep Neural Networks via Convex Duality](https://arxiv.org/pdf/2002.09773.pdf)

- Two regimes in neural network training
  - [Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability](https://arxiv.org/pdf/2103.00065.pdf)
  - [Super-Convergence with an Unstable Learning Rate](https://arxiv.org/pdf/2102.10734.pdf)
  - [The large learning rate phase of deep learning: the catapult mechanism](https://arxiv.org/pdf/2003.02218.pdf)
  - [The Two Regimes of Deep Network Training](https://arxiv.org/pdf/2002.10376.pdf)

- ~~Coresets in ML~~
  - ~~[GRAD-MATCH: A Gradient Matching Based Data Subset Selection for Efficient Learning](https://arxiv.org/pdf/2103.00123.pdf)~~
  - ~~[Coresets for Data-efficient Training of Machine Learning Models](https://arxiv.org/pdf/1906.01827.pdf)~~
  - ~~[Coresets for robust training of neural networks against noisy labels](https://arxiv.org/pdf/2011.07451.pdf)~~
  
- ~~Acceleration in non-convex optimization~~
  - ~~[Accelerated methods for non-convex optimization](https://arxiv.org/pdf/1611.00756.pdf)~~
  - ~~(More papers to be announced)~~
      
&nbsp;
&nbsp;
