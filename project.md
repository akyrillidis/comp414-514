---
layout: default
---

&nbsp;

[Proceedings 2019 - Part 1](/schedule/images/Proceedings2019_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2019 - Part 2](/schedule/images/Proceedings2019_Part2.pdf)

[Proceedings 2020 - Part 1](/schedule/images/Proceedings2020_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2020 - Part 2](/schedule/images/Proceedings2020_Part2.pdf)

[Proceedings 2021 - Part 1](/schedule/images/Proceedings2021_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2021 - Part 2](/schedule/images/Proceedings2021_Part2.pdf)

[Proceedings 2023 - Part 1](/schedule/images/Proceedings2023_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2023 - Part 2](/schedule/images/Proceedings2023_Part2.pdf)

&nbsp;

The course project can be categorized as a literature review, original research, or a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes an in-depth review and analysis of a paper (to be selected from a list of papers provided by the instructor or you after the instructor's approval). The review should provide an in-depth summary, exposition, and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current study with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner as possible**. **Deadline: February 7th (Friday).**

- Submit a one-page description of the project, what it is about, your opinion, what needs to be done (related papers to read), and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan for the next steps and what needs to be done by the end of the semester to finish the project. **Deadline: February 14th (Friday).**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (~5-10mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The report should be at least six pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester if it deserves publication.

### Suggested list of projects/papers (to be updated)

**Project ideas**
  
- The math and modeling behind AI in material science
  - [Crystal diffusion variational autoencoder for periodic material generation](https://arxiv.org/pdf/2110.06197)
  - [An autonomous laboratory for the accelerated synthesis of novel materials](https://www.nature.com/articles/s41586-023-06734-w.pdf)
  - [Scaling deep learning for materials discovery](https://www.nature.com/articles/s41586-023-06735-9.pdf)
  - [Can LLMs generate diverse molecules?](https://arxiv.org/pdf/2410.03138)

- Modernized view of learning rates in optimization methods
  - [Acceleration via fractal learning rate schedules](https://proceedings.mlr.press/v139/agarwal21a/agarwal21a.pdf)
  - [Super-acceleration with cyclical step-sizes](https://proceedings.mlr.press/v151/goujaud22a/goujaud22a.pdf)
  - [Provably Faster Gradient Descent via Long Steps](https://arxiv.org/pdf/2307.06324.pdf)

- Recent advances in adaptive methods in ML
  - [Adaptive proximal algorithms for convex optimization under local Lipschitz continuity of the gradient](https://arxiv.org/pdf/2301.04431.pdf)
  - [Stochastic polyak step-size for SGD: An adaptive learning rate for fast convergence](https://arxiv.org/pdf/2002.10542.pdf)
  - [Learning-Rate-Free Learning by D-Adaptation](https://arxiv.org/pdf/2301.07733.pdf)
  - [Automatic Gradient Descent: Deep Learning without Hyperparameters](https://arxiv.org/pdf/2304.05187.pdf)
  - [Adaptive FL with auto-tuned clients](https://arxiv.org/pdf/2306.11201.pdf)
  
- Sparse post-training pruning and update in Neural Network training
  - [i-SpaSP: Structured Neural Pruning via Sparse Signal Recovery](https://arxiv.org/pdf/2112.04905.pdf)
  - [Writeup provided by instructor - per request]()

- Quantum Approximate Optimization Algorithm from an Optimization perspective
  - [Writeup provided by instructor - per request]()

- Empirical evaluation of derivative-free optimization methods in quantum objectives
  - [Performance of the Quantum Approximate Optimization Algorithm on the Maximum Cut Problem](https://arxiv.org/pdf/1811.08419.pdf)
  - [Introductory Tutorial for SPSA and the Quantum Approximation Optimization Algorithm](https://arxiv.org/pdf/2106.01578.pdf)
  - [Global Solutions to Nonconvex Problems by Evolution of Hamilton-Jacobi PDEs](https://arxiv.org/pdf/2202.11014.pdf)
  - (probably more recent papers will be included...)
 
- Homotopy methods, graduated optimization, and quantum annealing
  - [On Graduated Optimization for Stochastic Non-Convex Problems](https://arxiv.org/pdf/1503.03712.pdf)
  - [Adiabatic quantum computing with parameterized quantum circuits](https://arxiv.org/pdf/2206.04373.pdf)
  - [Faster Convex Optimization: Simulated Annealing with an Efficient Universal Barrier](https://arxiv.org/pdf/1507.02528.pdf)

- Transformer alternatives
  - [Attention is all you need](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
  - [Overview of transformers](https://lilianweng.github.io/posts/2020-04-07-the-transformer-family/)
  - [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/pdf/2105.01601.pdf)
  - [Retentive Network: A Successor to Transformer for Large Language Models](https://arxiv.org/pdf/2307.08621.pdf)
 
- (Recent) advances in asynchrony on distributed SGD
  - [Learning Under Delayed Feedback: Implicitly Adapting to Gradient Delays](https://arxiv.org/pdf/2106.12261.pdf)
  - [Asynchronous Stochastic Optimization Robust to Arbitrary Delays](https://proceedings.neurips.cc/paper/2021/file/4b85256c4881edb6c0776df5d81f6236-Paper.pdf)
  - [Sharper Convergence Guarantees for Asynchronous SGD for Distributed and Federated Learning](https://arxiv.org/pdf/2206.08307.pdf)
  - [Asynchronous SGD Beats Minibatch SGD Under Arbitrary Delays](https://arxiv.org/pdf/2206.07638.pdf)
 
- Review of classical continual learning methods
  - [iCaRL: Incremental Classifier and Representation Learning](https://arxiv.org/pdf/1611.07725.pdf)
  - [End-to-End Incremental Learning](https://arxiv.org/pdf/1807.09536.pdf)
  - [Efficient Lifelong Learning with A-GEM](https://arxiv.org/pdf/1812.00420.pdf)
  - [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/pdf/2004.10964.pdf)
  - [Gradient Episodic Memory for Continual Learning](https://arxiv.org/pdf/1706.08840.pdf)
  - [Cold Start Streaming Learning for Deep Networks](https://arxiv.org/pdf/2211.04624.pdf)

- Review of modern continual learning methods
  - [One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning](https://arxiv.org/pdf/2311.12048.pdf)
  - [Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt](https://arxiv.org/pdf/2305.11186.pdf)
  - [FedJETs: Efficient Just-In-Time Personalization with Federated Mixture of Experts](https://arxiv.org/pdf/2306.08586.pdf)
  - [Sweeping Heterogeneity with Smart MoPs: Mixture of Prompts for LLM Task Adaptation](https://arxiv.org/pdf/2310.02842.pdf)

- Review of ML models on weather forecasting
  - [Learning skillful medium-range global weather forecasting](https://arxiv.org/pdf/2212.12794.pdf)
  - [ClimaX: A foundation model for weather and climate](https://arxiv.org/pdf/2301.10343.pdf)
  - [FourCastNet: Accelerating Global High-Resolution Weather Forecasting using Adaptive Fourier Neural Operators](https://arxiv.org/pdf/2208.05419.pdf)
  - [WeatherBench 2: A benchmark for the next generation of data-driven global weather models](https://arxiv.org/pdf/2308.15560.pdf)
  - [FENGWU: PUSHING THE SKILLFUL GLOBAL MEDIUM-RANGE WEATHER FORECAST BEYOND 10 DAYS LEAD](https://arxiv.org/pdf/2304.02948.pdf)
  - (probably more papers will be needed...)

- Review of large-scale ML models in AI
  - [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)
  - [Branch-Train-Merge training of expert LLMs](https://arxiv.org/pdf/2208.03306.pdf)
  - [Training a language model on a single GPU in one day](https://arxiv.org/pdf/2212.14034.pdf)
  - [Scaling ViT to 22B](https://arxiv.org/pdf/2302.05442.pdf)
  - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
  - [BloombergGPT](https://arxiv.org/pdf/2303.17564.pdf)
  - [Textbooks Are All You Need](https://arxiv.org/pdf/2306.11644.pdf)
  - (probably more papers will be needed...)

- Literature review on adapters in neural network training and optimization
  - [Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)
  - [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf)
  - [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/pdf/2106.09685.pdf)
  - [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf)

- Literature review of recent developments on Frank-Wolfe methods
  - [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
  - (More papers to be announced)

- Recent advances in acceleration methods
  - [Provable Acceleration of Heavy Ball beyond Quadratics for a Class of Polyak-≈Åojasiewicz Functions when the Non-Convexity is Averaged-Out](https://arxiv.org/pdf/2206.11872.pdf)
  - [Accelerated Convergence of Nesterov's Momentum for Deep Neural Networks under Partial Strong Convexity](https://arxiv.org/pdf/2306.08109.pdf)
  - [Provable non-accelerations of the heavy-ball method](https://arxiv.org/pdf/2307.11291.pdf)

- Review of Byzantine Distributed optimization
  - [Machine learning with adversaries: Byzantine tolerant gradient descent](https://papers.nips.cc/paper_files/paper/2017/file/f4b9ec30ad9f68f89b29639786cb62ef-Paper.pdf)
  - [Byzantine-resilient SGD in high dimensions on heterogeneous data](https://arxiv.org/pdf/2005.07866.pdf)
  - [The hidden vulnerability of distributed learning in byzantium](https://arxiv.org/pdf/1802.07927.pdf)
  - [Byzantine machine learning made easy by resilient averaging of momentums](https://arxiv.org/pdf/2205.12173.pdf)
  - [An equivalence between data poisoning and byzantine gradient attacks](https://arxiv.org/pdf/2202.08578.pdf)
  - [Byzantine-robust learning on heterogeneous datasets via resampling](https://arxiv.org/pdf/2006.09365.pdf)

- Review on efficient distributed protocols: independent subnetwork training (IST)
  - [Distributed learning of fully connected neural networks using independent subnet training](https://par.nsf.gov/servlets/purl/10404274)
  - [GIST: Distributed training for large-scale graph convolutional networks](https://link.springer.com/article/10.1007/s41468-023-00127-8)
  - [Resist: Layer-wise decomposition of resnets for distributed training](https://proceedings.mlr.press/v180/dun22a/dun22a.pdf)
  - [Efficient and Light-Weight Federated Learning via Asynchronous Distributed Dropout](https://proceedings.mlr.press/v206/dun23a/dun23a.pdf)
  - [Federated Learning Over Images: Vertical Decompositions and Pre-Trained Backbones Are Difficult to Beat](https://openaccess.thecvf.com/content/ICCV2023/papers/Hu_Federated_Learning_Over_Images_Vertical_Decompositions_and_Pre-Trained_Backbones_Are_ICCV_2023_paper.pdf)

- Review of theoretical results for various pruning methods
  - [On the convergence of shallow neural network training with randomly masked neurons](https://arxiv.org/pdf/2112.02668.pdf)
  - [How much pre-training is enough to discover a good subnetwork?](https://arxiv.org/pdf/2108.00259.pdf)
  - [Strong Lottery Ticket Hypothesis with epsilon‚Äìperturbation](https://proceedings.mlr.press/v206/xiong23a/xiong23a.pdf)

&nbsp;
&nbsp;
