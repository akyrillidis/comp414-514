---
layout: default
---

&nbsp;

[Proceedings 2019 - Part 1](/schedule/images/Proceedings2019_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2019 - Part 2](/schedule/images/Proceedings2019_Part2.pdf)

[Proceedings 2020 - Part 1](/schedule/images/Proceedings2020_Part1.pdf) &emsp;&emsp;&emsp;   [Proceedings 2020 - Part 2](/schedule/images/Proceedings2020_Part2.pdf)

&nbsp;

The course project can be categorized as a literature review, as original research, or as a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes depth review and analysis of a paper (to be selected from a list of papers, provided by the instructor or by you, after instroctor's approval). The review should provide an in-depth summary, exposition and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current research with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner possible**. **Deadline: September 21st.**

- Submit a one-page description of the project, what is it about, your opinion, what needs to be done (related papers to read) and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan on what are the next steps and what needs to be done till the end of the semester to finish the project. **Deadline: October 5th.**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (~5mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The length of the report should be at least 6 pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester, if it deserves publication.

### Suggested list of projects/papers (to be updated)

**Project ideas**

- Contrastive learning
  - [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/pdf/2002.05709.pdf)
  - [Deep contextualized word representations](https://arxiv.org/pdf/1802.05365.pdf)
  - [Supervised constrastive learning](https://arxiv.org/pdf/2004.11362.pdf)
  - [Momentum Contrast for Unsupervised Visual Representation Learning](https://arxiv.org/pdf/1911.05722.pdf)
  - [Improved Baselines with Momentum Contrastive Learning](https://arxiv.org/pdf/2003.04297.pdf)

- Ensemble training and higher variance models
  - [Rethinking Bias-Variance Trade-off for Generalization of Neural Networks](https://arxiv.org/pdf/2002.11328.pdf)
  - [More Data Can Hurt for Linear Regression: Sample-wise Double Descent](https://arxiv.org/pdf/1912.07242.pdf)

- Acceleration in low-rank factorization problems
  - [Accelerated Factored Gradient Descent for Low-Rank Matrix Factorization](http://proceedings.mlr.press/v108/zhou20b/zhou20b.pdf)
  - [On the optimization of deep networks - implicit acceleration by overparameterization](https://arxiv.org/pdf/1802.06509.pdf)

- Theory on simplistic ResNet models
  - [On the Global Convergence of Training Deep Linear ResNets](https://openreview.net/pdf?id=HJxEhREKDH)
  - [Identity Matters in Deep Learning](https://arxiv.org/pdf/1611.04231.pdf)
  
- Practical algorithms for SDPs
  - [Smoothed analysis for low-rank solutions to SDPs in quadratic penalty form](https://arxiv.org/pdf/1803.00186.pdf)
  - [The mixing method - low-rank coordinate descent for semidefinite programming with diagonal constraints](https://arxiv.org/pdf/1706.00476.pdf)

**Literature reviews**

- The use of momentum in stochastic methods
  - [Convergence of a Stochastic Gradient Method with Momentum for Non-Smooth Non-Convex Optimization](https://proceedings.icml.cc/static/paper_files/icml/2020/4205-Paper.pdf)
  - [On the convergence of the Stochastic Heavy Ball Method](https://arxiv.org/pdf/2006.07867.pdf)
  - [On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings](https://arxiv.org/pdf/2002.12414.pdf)

- Literature review of recent developments on Frank-Wolfe methods
  - [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
  - (More papers to be announced)
  
- Recent algorithmic advances on explainable AI
  - [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/pdf/2004.13912.pdf)

- Derivative free optimization in ML
  - [A Theoretical and Empirical Comparison of Gradient Approximations in Derivative-Free Optimization](https://arxiv.org/pdf/1905.01332.pdf)
  - [An Accelerated DFO Algorithm for Finite-sum Convex Functions](https://arxiv.org/pdf/2007.03311.pdf)
  
- Different theory techniques for shallow neural networks
  - [Globally optimal gradient descent for a convnet with Gaussian inputs](https://arxiv.org/pdf/1702.07966.pdf)
  - [Learning ReLU Networks via alternating minimization](https://arxiv.org/pdf/1806.07863.pdf)
  - [Learning ReLUs via Gradient Descent](https://arxiv.org/pdf/1705.04591.pdf)

- Acceleration in non-convex optimization
  - [Accelerated methods for non-convex optimization](https://arxiv.org/pdf/1611.00756.pdf)
  - (More papers to be announced)
  
- Discrete optimizers as layers in neural networks
  - [SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver](https://arxiv.org/pdf/1905.12149.pdf)
  - [Differentiation of Blackbox Combinatorial Solvers](https://openreview.net/pdf?id=BkevoJSYPB)
  - (More papers to be announced)
  
- Second-order optimization methods in ML
  - [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)
  - [Stochastic Block BFGS: Squeezing More Curvature out of Data](https://arxiv.org/pdf/1603.09649.pdf)
  - [RSN: Randomized Subspace Newton](https://arxiv.org/pdf/1905.10874.pdf)
  
&nbsp;
&nbsp;
