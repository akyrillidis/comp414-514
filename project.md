---
layout: default
---

&nbsp;

The course project can be categorized as a literature review, as original research, or as a literature review that ends up as original research (there is flexibility to that).

- **Literature review.** This includes depth review and analysis of a paper (to be selected from a list of papers, provided by the instructor or by you, after instroctor's approval). The review should provide an in-depth summary, exposition and discussion of the paper (which will often include reading other related papers on that subject).

- **Original research.** You are strongly encouraged to combine your current research with the course project. Otherwise, the instructor will provide some ideas to follow. It can be either theoretical or experimental. 

### Milestones

- Pick a project the **sooner possible**.

- Submit a one-page description of the project, what is it about, your opinion, what needs to be done (related papers to read) and whether you have any ideas to improve the ideas involved. Describe why they are important or interesting, and provide some appropriate references. If it is original research, provide a plan on what are the next steps and what needs to be done till the end of the semester to finish the project. **Deadline: September 10th.**

- We will probably have in-class presentations towards the end of the semester. These will be spotlight talks (5mins). Prepare an oral presentation with slides. Focus on high-level ideas, and leave most technical details to your report.

- A written report. A LaTeX template will be provided (most probably in ICML format). The length of the report should be at least 6 pages (excluding references). **Deadline: End of the semester.** Note that the project can continue beyond the end of the semester, if it deserves publication.

### Suggested list of papers (to be updated)

- [Practical coresets constructions for machine learning](https://arxiv.org/pdf/1703.06476.pdf)
- [Near-optimal bounds for phase synchronization](https://arxiv.org/pdf/1703.06605.pdf)
- [Online learning of quantum states](https://arxiv.org/pdf/1802.09025.pdf)
- [Distributed representations of words and phrases and their compositionality](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)
- [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)
- [Identity matters in deep learning](https://arxiv.org/pdf/1611.04231.pdf)
- [Globally optimal gradient descent for a convnet with Gaussian inputs](https://arxiv.org/pdf/1702.07966.pdf)
- [Learning ReLUs via Gradient Descent](https://arxiv.org/pdf/1705.04591.pdf)
- [First-order methods of smooth convex optimization with inexact oracle](http://www.optimization-online.org/DB_FILE/2010/12/2865.pdf)
- [A geometric alternative to Nesterov's accelerated gradient descent](https://arxiv.org/pdf/1506.08187.pdf)
- [Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent](https://arxiv.org/pdf/1106.5730.pdf)
- [Revisiting Frank-Wolfe: Projection-Free Sparse Convex Optimization](http://m8j.net/math/revisited-FW.pdf)
- [A differential equation for modeling nesterov's accelerated gradient method - Theory and Insights](http://jmlr.org/papers/volume17/15-084/15-084.pdf)
- [Escaping from saddle points - online SGD for tensor decomposition](https://arxiv.org/pdf/1503.02101.pdf)
- [Smoothed analysis for low-rank solutions to SDPs in quadratic penalty form](https://arxiv.org/pdf/1803.00186.pdf)
- [Nonlinear inductive matrix completion based on one-layer neural networks](https://arxiv.org/pdf/1805.10477.pdf)
- [The mixing method - low-rank coordinate descent for semidefinite programming with diagonal constraints](https://arxiv.org/pdf/1706.00476.pdf)
- [On the optimization of deep networks - implicit acceleration by overparameterization](https://arxiv.org/pdf/1802.06509.pdf)
- [Learning ReLU Networks via alternating minimization](https://arxiv.org/pdf/1806.07863.pdf)
- [Accelerated methods for non-convex optimization](https://arxiv.org/pdf/1611.00756.pdf)
- [How to escape saddle points efficiently](https://arxiv.org/pdf/1703.00887.pdf)
- [An alternative view - when does SGD escape local minima?](https://arxiv.org/pdf/1802.06175.pdf)
- [Analysis and Design of Optimization Algorithms via Integral Quadratic Constraints](https://arxiv.org/pdf/1408.3595.pdf)
- [Regularizing linear inverse problems with convolutional neural networks](https://arxiv.org/pdf/1907.03100.pdf)
- [Accelerated Stochastic Power Iteration](https://arxiv.org/pdf/1707.02670.pdf)
- [SATNet: Bridging deep learning and logical reasoning using a differentiable satisfiability solver](https://arxiv.org/pdf/1905.12149.pdf)

&nbsp;
&nbsp;
