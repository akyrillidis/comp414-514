---
layout: default
---

&nbsp;

## Course literature

### Book references

1. Nocedal and S. Wright. Numerical optimization. Springer Science & Business Media, 2006.

2. Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.

3. S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

4. D. Bertsekas. Convex optimization algorithms. Athena Scientific Belmont, 2015.

5. S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231–357, 2015.

### Papers

1. [Efficient Projections onto the l1-Ball for Learning in High Dimensions](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)

2. [Stay on path: PCA along graph paths](http://proceedings.mlr.press/v37/asteris15.pdf)

3. [CUR matrix decompositions for improved data analysis](http://www.pnas.org/content/106/3/697.full.pdf)

4. [Simple and Deterministic Matrix Sketching](https://arxiv.org/pdf/1206.0594.pdf)

5. [Provable deterministic leverage score sampling](https://arxiv.org/abs/1404.1530)

5. [Linear convergence of gradient and proximal-gradient methods under the Polyak-Łojasiewicz condition](https://arxiv.org/abs/1608.04636)

6. [Conditional Gradient Algorithms for Rank-One Matrix Approximations with a Sparsity Constraint
](https://arxiv.org/pdf/1107.1163.pdf)

7. [Frank-Wolfe with Subsampling Oracle](https://arxiv.org/pdf/1803.07348.pdf)

8. [Linear convergence of a Frank-Wolfe type algorithm over trace-norm balls](https://papers.nips.cc/paper/7199-linear-convergence-of-a-frank-wolfe-type-algorithm-over-trace-norm-balls.pdf)

9. [Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored
Approximate Curvature for Deep Convolutional Neural Networks](https://arxiv.org/pdf/1811.12019.pdf)

10. [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)

11. [Adaptive Restart for Accelerated Gradient Schemes](https://arxiv.org/abs/1204.3982)

12. [Why momentum really works](https://distill.pub/2017/momentum/)

13. [A geometric alternative to Nesterov's accelerated gradient descent](https://arxiv.org/abs/1506.08187)

14. [A variational perspective on accelerated methods in optimization](https://arxiv.org/pdf/1603.04245.pdf)

15. [Large scale machine learning with SGD](http://khalilghorbal.info/assets/spa/papers/ML_GradDescent.pdf)

16. [Accelerating SGD using predictive variance reduction](https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf)

17. [Coordinate descent algorithms](http://www.optimization-online.org/DB_FILE/2014/12/4679.pdf)

18. [Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm](https://arxiv.org/pdf/1310.5715.pdf)

19. [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)

20. [CoSaMP: Iterative signal recovery from incomplete and inaccurate samples](https://www.sciencedirect.com/science/article/pii/S1063520308000638)

21. [Compressed sensing using generative models](https://arxiv.org/pdf/1703.03208.pdf)

22. [A Nearly-Linear Time Framework for graph-structured sparsity](http://proceedings.mlr.press/v37/hegde15.pdf)

23. [Deep image prior](https://arxiv.org/pdf/1711.10925.pdf)
