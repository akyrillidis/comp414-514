---
layout: default
---

&nbsp;

## Course literature

### Book references

1. Nocedal and S. Wright. Numerical optimization. Springer Science & Business Media, 2006.

2. Y. Nesterov. Introductory lectures on convex optimization: A basic course, volume 87. Springer Science & Business Media, 2013.

3. S. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.

4. D. Bertsekas. Convex optimization algorithms. Athena Scientific Belmont, 2015.

5. S. Bubeck. Convex optimization: Algorithms and complexity. Foundations and Trends in Machine Learning, 8(3-4):231–357, 2015.

### Papers

- [Efficient Projections onto the l1-Ball for Learning in High Dimensions](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)
- [Stay on path: PCA along graph paths](http://proceedings.mlr.press/v37/asteris15.pdf)
- [CUR matrix decompositions for improved data analysis](http://www.pnas.org/content/106/3/697.full.pdf)
- [Simple and Deterministic Matrix Sketching](https://arxiv.org/pdf/1206.0594.pdf)
- [Provable deterministic leverage score sampling](https://arxiv.org/abs/1404.1530)
- [EigenGame: PCA as a Nash equilibrium](https://arxiv.org/pdf/2010.00554.pdf)
- [Linear convergence of gradient and proximal-gradient methods under the Polyak-Łojasiewicz condition](https://arxiv.org/abs/1608.04636)
- [Conditional Gradient Algorithms for Rank-One Matrix Approximations with a Sparsity Constraint](https://arxiv.org/pdf/1107.1163.pdf)
- [Frank-Wolfe with Subsampling Oracle](https://arxiv.org/pdf/1803.07348.pdf)
- [Linear convergence of a Frank-Wolfe type algorithm over trace-norm balls](https://papers.nips.cc/paper/7199-linear-convergence-of-a-frank-wolfe-type-algorithm-over-trace-norm-balls.pdf)
- [Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature for Deep Convolutional Neural Networks](https://arxiv.org/pdf/1811.12019.pdf)
- [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)
- [Adaptive Restart for Accelerated Gradient Schemes](https://arxiv.org/abs/1204.3982)
- [Why momentum really works](https://distill.pub/2017/momentum/)
- [A geometric alternative to Nesterov's accelerated gradient descent](https://arxiv.org/abs/1506.08187)
- [A variational perspective on accelerated methods in optimization](https://arxiv.org/pdf/1603.04245.pdf)
- [Large scale machine learning with SGD](http://khalilghorbal.info/assets/spa/papers/ML_GradDescent.pdf)
- [Accelerating SGD using predictive variance reduction](https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf)
- [Coordinate descent algorithms](http://www.optimization-online.org/DB_FILE/2014/12/4679.pdf)
- [Stochastic Gradient Descent, Weighted Sampling, and the Randomized Kaczmarz algorithm](https://arxiv.org/pdf/1310.5715.pdf)
- [On the importance of initialization and momentum in deep learning](http://www.cs.toronto.edu/~fritz/absps/momentum.pdf)
- [CoSaMP: Iterative signal recovery from incomplete and inaccurate samples](https://www.sciencedirect.com/science/article/pii/S1063520308000638)
- [Compressed sensing using generative models](https://arxiv.org/pdf/1703.03208.pdf)
- [A Nearly-Linear Time Framework for graph-structured sparsity](http://proceedings.mlr.press/v37/hegde15.pdf)
- [Deep image prior](https://arxiv.org/pdf/1711.10925.pdf)
