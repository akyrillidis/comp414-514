---
layout: default
---

&nbsp;

## Course literature

### Book references

### Papers

1. [Efficient Projections onto the l1-Ball for Learning in High Dimensions](https://stanford.edu/~jduchi/projects/DuchiShSiCh08.pdf)

2. [Stay on path: PCA along graph paths](http://proceedings.mlr.press/v37/asteris15.pdf)

3. [CUR matrix decompositions for improved data analysis](http://www.pnas.org/content/106/3/697.full.pdf)

4. [Simple and Deterministic Matrix Sketching](https://arxiv.org/pdf/1206.0594.pdf)

5. [Linear convergence of gradient and proximal-gradient methods under the Polyak-≈Åojasiewicz condition](https://arxiv.org/abs/1608.04636)

6. [Conditional Gradient Algorithms for Rank-One Matrix Approximations with a Sparsity Constraint
](https://arxiv.org/pdf/1107.1163.pdf)

7. [Frank-Wolfe with Subsampling Oracle](https://arxiv.org/pdf/1803.07348.pdf)

8. [Linear convergence of a Frank-Wolfe type algorithm over trace-norm balls](https://papers.nips.cc/paper/7199-linear-convergence-of-a-frank-wolfe-type-algorithm-over-trace-norm-balls.pdf)

9. [Large-Scale Distributed Second-Order Optimization Using Kronecker-Factored
Approximate Curvature for Deep Convolutional Neural Networks](https://arxiv.org/pdf/1811.12019.pdf)

10. [Deep learning via Hessian-free optimization](http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf)

11. [Adaptive Restart for Accelerated Gradient Schemes](https://arxiv.org/abs/1204.3982)

12. [Why momentum really works](https://distill.pub/2017/momentum/)

13. [A geometric alternative to Nesterov's accelerated gradient descent](https://arxiv.org/abs/1506.08187)

14. [A variational perspective on accelerated methods in optimization](https://arxiv.org/pdf/1603.04245.pdf)
