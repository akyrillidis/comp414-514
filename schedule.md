---
layout: default
---

&nbsp;


| # | Date  | Topic  | Notes | Lecture | Notebook  |
|-|-|-|-|-|-|
| | | **Part I: Basic gradient methods** | | | |
| 1 | 08.(23/25) | Intro & Preliminaries  | [.pdf](/schedule/images/chapter1.pdf) | [.pdf](/schedule/images/Lecture 1.pdf) | [.ipynb](/schedule/images/Chapter 1a.ipynb) [.ipynb](/schedule/images/Chapter 1b.ipynb)
| 2 | 08.30-09.(01/06) | Gradient method | [.pdf](/schedule/images/chapter2.pdf)  | [.pdf](/schedule/images/Lecture 2.pdf) | [.ipynb](/schedule/images/Chapter 2.ipynb) |
| 3 | 09.(08/13/15) | Gradient method & Convexity | [.pdf](/schedule/images/chapter3.pdf)  | [.pdf](/schedule/images/Lecture 3.pdf) | [.ipynb](/schedule/images/Chapter 3.ipynb) |
| 4 | 09.(20/22)  | Conditional gradient (Frank-Wolfe) | [.pdf]()  | [.pdf]() | [.ipynb]() |
| | | **Part II: Going faster than basic gradient descent** | | | |
| 5 | 09.(27/29) | Beyond first-order methods | [.pdf]()  | [.pdf]() | [.ipynb]() |
| 6 | 10.(04/06) | Momemtum acceleration | [.pdf]()  | [.pdf]() | [.ipynb]() |
| 7 | 10.(13/18) | Stochastic motions in gradient descent | [.pdf]()  | [.pdf]() | [.ipynb]() |
| | | **Part III: Provable non-convex optimization** | | | |
| 8 | 10.(20/25/27) | Sparse feature selection and recovery | [.pdf]() / [.pdf]() | [.pdf]() | [.ipynb]() |
|  |  | | (Handwritten / Latex) |  | |
| 9 | 11.(01/03) | Low-rank recovery | [.pdf]() / [.pdf]() | [.pdf]() | [.ipynb]() |
|  |  | | (Handwritten / Latex) |  | |
| | | **Part IV: Optimization methods in modern ML** | | | |
| 10 | 11.(08/10) | Landscape properties of general functions |  [.pdf]()  | [.pdf]() | --- |
| 11 | 11.(15/17) | Algorithms for NN training | [.pdf]() | [.pdf]() | --- |
| 12 | 11.(22) | To be decided | [.pdf]() | [.pdf]() | --- |
| 13 | 11.29-12.01 | Project presentations  | ---  | ---  | --- |
| | | **Part V: Final exam** | | | |

&nbsp;
&nbsp;
