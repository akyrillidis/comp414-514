---
layout: default
---

&nbsp;


| # | Date  | Topic  | Notes | Lecture | Notebook  |
|-|-|-|-|-|-|
| | | **Part I: Basic gradient methods** | | | |
| 1 | 08.(26/31) | Intro & Preliminaries  | [.pdf](/schedule/images/chapter1.pdf) | [.pdf](/schedule/images/Lecture 1.pdf) | [.ipynb](/schedule/images/Chapter 1a.ipynb) [.ipynb](/schedule/images/Chapter 1b.ipynb)
| 2 | 09.(02/07) | Gradient method | [.pdf](/schedule/images/chapter2.pdf)  | [.pdf](/schedule/images/Lecture 2.pdf) | [.ipynb](/schedule/images/Chapter 2.ipynb) |
| 3 | 09.(09/16/21) | Gradient method & Convexity | [.pdf](/schedule/images/chapter3.pdf)  | [.pdf](/schedule/images/Lecture 3.pdf) | [.ipynb](/schedule/images/Chapter 3.ipynb) |
| 4 | 09.(23/28)  | Conditional gradient (Frank-Wolfe) | [.pdf](/schedule/images/chapter4.pdf)  | [.pdf](/schedule/images/Lecture 4.pdf) | [.ipynb](/schedule/images/Chapter 4.ipynb) |
| | | **Part II: Going faster than basic gradient descent** | | | |
| 5 | 09.30-10.05 | Beyond first-order methods | [.pdf]()  | [.pdf]() | [.ipynb]() |
| | | (videos will be provided) |  | |  |
| 6 | 10.07 | Momemtum acceleration | [.pdf]()  | [.pdf]() | [.ipynb]() |
| | | (videos will be provided) |  | |  |
| 7 | 10.(14/19) | Stochastic motions in gradient descent | [.pdf]()  | [.pdf]() | [.ipynb]() |
| | | **Part III: Provable non-convex optimization** | | | |
| 8 | 10.(21/26/28) | Sparse feature selection and recovery | [.pdf]() / [.pdf]() | [.pdf]() | [.ipynb]() |
|  |  | | (Handwritten / Latex) |  | |
| 9 | 11.(02/04) | Low-rank recovery | [.pdf]() / [.pdf]() | [.pdf]() | [.ipynb]() |
|  |  | | (Handwritten / Latex) |  | |
| | | **Part IV: Optimization methods in modern ML** | | | |
| 10 | 11.(09/11) | Landscape properties of general functions | [.pdf]()  | [.pdf]() | --- |
| 11 | 11.(16/18) | Distributed computations with GD | [.pdf]() | [.pdf]() | --- |
| 12 | 11.(23/30) | TBD  | ---  | ---  | --- |
| 13 | 12.02 | TBD  | ---  | ---  | --- |
| | | **Part V: Final exam** | | | |

&nbsp;
&nbsp;
