---
layout: page
title: About
permalink: /about/
---


The main focus of the course will be on smooth optimization techniques, with applications in machine learning and artificial intelligence. The course will introduce the basics of algorithms on continuous optimization, starting from the classical gradient descent algorithm in convex optimization, towards more sophisticated approaches in non-convex scenarios. The course will explore the fundamental theory, algorithms, complexity and approximations in nonlinear optimization. 

The prerequisites are linear algebra, multivariate calculus, probability and statistics. AI/Machine learning courses are not necessary but highly recommended.

Outcomes:
After successful attendance, students are expected to:
(i) have a good understanding of theory involved in optimization, via machine learning/AI applications
(ii) understand the differences of and the reasoning/logic behind optimization algorithms, such as SGD, adaptive methods (Adam, RMSProp, Adagrad, etc), and second order methods.
(iii) have a good understanding of standard convex optimization techniques, both in theory and practice.
(iv) have a good understanding of the differences / difficulties of convex and non-convex optimization.
(v)  have a good comprehension how optimization plays a key role in different areas of ML/AI/SP.
(vi)  be able to read and review advanced papers on similar subjects.
